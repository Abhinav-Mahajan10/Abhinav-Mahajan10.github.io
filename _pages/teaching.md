---
layout: page
permalink: /teaching/
title: Teaching
description: 
nav: true
nav_order: 6
---

### Teaching Assistant – Machine Learning (AI-511) & Visual Learning (AI-825)

In Fall 2023, I served as a **Teaching Assistant** for **Machine Learning (AI-511)** and in Spring 2024, for **Visual Learning (AI-825)**  

My TA role in **Machine Learning** was highly hands-on. In addition to holding office hours and grading a cohort of more than 100 students, our team of five TAs jointly conducted weekly recap lectures paired with practical coding sessions of the theory taught in class. We also organized multiple Kaggle competitions, and were responsible for designing and evaluating major course projects for our individual cohorts.

**Our Course Material:** [Machine Learning TA Resources for Fall 2023 on GitHub](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023)

Here is a breakdown of our weekly material. In each folder of the repo, there are coding session notebooks as well.

### Machine Learning (AI-511) – Weekly Breakdown

#### [Week 1: Preprocessing & Exploratory Data Analysis](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%201%20(07-08-23))
- Data cleaning and feature scaling  
- Handling missing values  
- Visual EDA for insights  

#### [Week 2: Linear Regression](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%202%20(14-08-23))
- Theory and implementation  
- Gradient Descent optimization  
- Model evaluation  

#### [Week 3: Maximum Likelihood Estimation & Bayesian Methods](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%203%20(01-09-23))
- Frequentist vs Bayesian inference  
- Priors, posteriors, and likelihoods  

#### [Week 4: Logistic Regression, K-Means, K-NN](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%204%20(08-09-23))
- Classification with Logistic Regression  
- Unsupervised learning: K-Means  
- Instance-based learning: K-NN  

#### [Week 5: PCA & Decision Trees](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%205%20(19-09-23))
- Dimensionality reduction with PCA  
- Interpretable models using Decision Trees  

#### [Week 6: Random Forests & Boosting](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%206%20(14-10-23))
- Ensemble learning techniques  
- Bagging with Random Forests  
- Boosting methods (AdaBoost, Gradient Boosting, CatBoost and some recent advances as well)  

#### [Week 7: Constrained Optimization](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%207%20(30-10-23))
- Lagrange multipliers  
- Regularization and constraints in ML  

#### [Week 8: Support Vector Machines](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%208%20(06-11-23))
- Margin maximization  
- Kernel trick for non-linear separation  

#### [Week 9: Neural Networks & PyTorch](https://github.com/Abhinav-Mahajan10/Machine-Learning-TA-Material-Fall-2023/tree/main/Session%209%20(20-11-23))
- Neural network basics  
- Implementing models in PyTorch  
- Training loops and backpropagation  

---

### My Project
Contested a kaggle compeition for my cohort on the task of [Weather Forecasting](https://www.kaggle.com/t/d596498e8f1d42acabf57b91ba2934b1), using a dataset with 23 features and 145,000 rows. The objective was to give students hands-on experience in feature selection through exploratory data analysis, applying model ensembling techniques, and performing hyperparameter tuning for optimal performance.

---
